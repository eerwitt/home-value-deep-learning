{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Test, Train and Cross Validation\n",
    "\n",
    "We need our list of images and corresponding metadata in a format which Caffe will understand. In order to do this we will separate out our set of images to be a test, train and cross validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* [Test, Train and Cross Validation](https://class.coursera.org/ml-005/lecture/61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ansible.runner\n",
    "import ansible.inventory\n",
    "\n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_from_webapp_training(webapp_training_output_file=\"./categorization/output.tsv\"):\n",
    "    \"\"\"\n",
    "    Generate training information from an output file which was created using the `categorization` webapp.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    webapp_training_output_file : str\n",
    "        Filename of the TSV generated using the webapp. TSV's format is \"zillow_id\\turl\\tcategory\\n\".\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple : (dict, list)\n",
    "        A tuple which includes a dictionary of categories with their corresponding index and a list of all the\n",
    "        training filenames.\n",
    "    \"\"\"\n",
    "    unique_tags = {}\n",
    "    results = []\n",
    "\n",
    "    # Caffe indexes syn_words based on 0 but the webapp starts at 1.\n",
    "    i = 0\n",
    "\n",
    "    with open(webapp_training_output_file, \"r\") as training_file:\n",
    "        for row in csv.DictReader(training_file, delimiter=\"\\t\"):\n",
    "            # Now we use the relative file URL\n",
    "            filename = row[\"url\"].split(\"/\")[-1]\n",
    "            category = row[\"category\"]\n",
    "\n",
    "            if unique_tags.get(category, None) is None:\n",
    "                unique_tags[category] = i\n",
    "                i += 1 \n",
    "\n",
    "            results.append((filename, unique_tags[category],))\n",
    "\n",
    "    # Randomize our list of train results (this will be randomized again by Caffe)\n",
    "    randomized = [\n",
    "        (random.random(), result) for result in results\n",
    "    ]\n",
    "    randomized.sort()\n",
    "    \n",
    "    # Remove the random value from the train result\n",
    "    train = map(lambda r: r[1], randomized)\n",
    "\n",
    "    return (unique_tags, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_training_files(unique_tags, train, computed_dir=\"./computed\"):\n",
    "    \"\"\"\n",
    "    Write the files Caffe expects {syn_words.txt, train.txt, val.txt} which are all space delimited files\n",
    "    with no headers.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    unique_tags : dict\n",
    "        Each unique tag and their integer index.\n",
    "    train : list(tuple(str,int,))\n",
    "        List of all training files which are stored as a filename and the index of the category\n",
    "        related to that file.\n",
    "    computed_dir : str\n",
    "        We will store all this information in a directory called \"computed\" so that we keep it\n",
    "        separate from the actual models being generated and the original information.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple : (str,str,str,)\n",
    "        The filenames which were created: (syn_words_filename, train_filename, val_filename,)\n",
    "    \"\"\"\n",
    "    syn_words_filename = \"{computed_dir}/syn_words.txt\".format(computed_dir=computed_dir)\n",
    "    train_filename = \"{computed_dir}/train.txt\".format(computed_dir=computed_dir)\n",
    "    val_filename = \"{computed_dir}/val.txt\".format(computed_dir=computed_dir)\n",
    "    \n",
    "    open(syn_words_filename, \"w\").close()\n",
    "    for k, v in unique_tags.iteritems():\n",
    "        with open(syn_words_filename, \"a\") as t:\n",
    "            t.write(\"%s %s\\n\" % (v, k))\n",
    "\n",
    "    split = int(round(len(train) * 0.2))\n",
    "    open(train_filename, \"w\").close()\n",
    "    for line in train[split:]:\n",
    "        with open(train_filename, \"a\") as t:\n",
    "            t.write(\"%s %s\\n\" % line)\n",
    "\n",
    "    open(val_filename, \"w\").close()\n",
    "    for line in train[:split]:\n",
    "        with open(val_filename, \"a\") as t:\n",
    "            t.write(\"%s %s\\n\" % line)\n",
    "    \n",
    "    return (syn_words_filename, train_filename, val_filename,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sync the computed directory to the caffe instance which will be doing this run.\n",
    "def sync_computed(instance_ip_address, computed_dir):\n",
    "    \"\"\"\n",
    "    Upload the computed directory to the instance we are about to run caffe on.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    instance_ip_address : str\n",
    "        The IP Address of the instance where Caffe will be ran.\n",
    "    computed_dir : str\n",
    "        Directory which keep all the computed files which were generated for unique tags,\n",
    "        training and cross validation.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Output from the ansible runner for the given instance.\n",
    "    \"\"\"\n",
    "    hosts = [instance_ip_address]\n",
    "\n",
    "    inventory = ansible.inventory.Inventory(hosts)\n",
    "    ansible_runner = ansible.runner.Runner(\n",
    "        module_name=\"synchronize\",\n",
    "        module_args=\"src={computed_dir} dest=./\".format(\n",
    "            computed_dir=computed_dir),\n",
    "        timeout=5,\n",
    "        inventory=inventory,\n",
    "        remote_user=\"ubuntu\"\n",
    "    )\n",
    "\n",
    "    out = ansible_runner.run()\n",
    "    if not out[\"contacted\"].get(instance_ip_address, None):\n",
    "        raise Exception(\"No response information from instance :/\")\n",
    "    \n",
    "    return out[\"contacted\"][instance_ip_address]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'changed': False,\n",
       " u'cmd': u'rsync --delay-updates -F --compress --archive --rsh \\'ssh  -S none -o StrictHostKeyChecking=no\\' --out-format=\\'<<CHANGED>>%i %n%L\\' \"./prototxt\" \"ubuntu@174.129.71.20:./\"',\n",
       " 'invocation': {'module_args': u'src=./prototxt dest=./',\n",
       "  'module_complex_args': {},\n",
       "  'module_name': 'synchronize'},\n",
       " u'msg': u'',\n",
       " u'rc': 0,\n",
       " u'stdout_lines': []}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the tags and training data then write them to local files.\n",
    "unique_tags, train = generate_from_webapp_training(webapp_training_output_file=\"./categorization/output.tsv\")\n",
    "\n",
    "computed_data_dir = \"./computed\"\n",
    "write_training_files(unique_tags, train, computed_dir=computed_data_dir)\n",
    "\n",
    "instance_ip = \"174.129.71.20\"\n",
    "sync_computed(instance_ip, computed_data_dir)\n",
    "sync_computed(instance_ip, \"./images\")\n",
    "sync_computed(instance_ip, \"./prototxt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Exterior': 0, 'Garden': 2, 'Interior': 1}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ISxzdiqn5yox4i1000000000.jpg', 1),\n",
       " ('ISz0m8d8ev9jmr.jpg', 0),\n",
       " ('ISxz9nzak77pyh1000000000.jpg', 0),\n",
       " ('ISdsuaf1rml8zh1000000000.jpg', 1),\n",
       " ('IS5msywumedbzh1000000000.jpg', 1)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
